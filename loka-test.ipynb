{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "from datetime import datetime\n",
    "from geopy.distance import geodesic\n",
    "from pyspark.sql.types import FloatType, StructType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=FloatType())\n",
    "def geodesic_udf(a, b):\n",
    "    return geodesic(a, b).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Loka Application\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-------+---------------+\n",
      "|                  at|                data| event|     on|organization_id|\n",
      "+--------------------+--------------------+------+-------+---------------+\n",
      "|2019-06-01 19:17:...|{null, bac5188f-6...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 3a3eb23a-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, f06eb89c-a...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, f0b87796-b...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, e641b45f-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 9152c5d8-7...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 949798fc-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 9d6a8840-d...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 3b0640d6-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 98c8b8cb-7...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, d6880741-a...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, d759fc35-b...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 49c02de6-2...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, b4dac3b4-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 46952066-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, ae413d00-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, a38edd77-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 804ac7f0-c...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 46f718d4-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 8efb00c5-3...|update|vehicle|         org-id|\n",
      "+--------------------+--------------------+------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"mergeSchema\", \"true\").json(\"/tmp/loka-data/*\")\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"at\", F.to_timestamp(df.at, timestamp_format))\n",
    "    .withColumn(\"date_start\", F.to_timestamp(df.data.start, timestamp_format))\n",
    "    .withColumn(\"date_finish\", F.to_timestamp(df.data.finish, timestamp_format))\n",
    "    .withColumn(\n",
    "        \"date_location_at\", F.to_timestamp(df.data.location.at, timestamp_format)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"data\",\n",
    "        F.struct(\n",
    "            \"data.*\",\n",
    "            \"date_start\",\n",
    "            \"date_finish\",\n",
    "            \"date_location_at\",\n",
    "        ),\n",
    "    )\n",
    "    .drop(\"date_start\")\n",
    "    .drop(\"date_finish\")\n",
    "    .drop(\"date_location_at\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.data.date_location_at.isNotNull()).select(\"data.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-------+---------------+\n",
      "|                  at|                data| event|     on|organization_id|\n",
      "+--------------------+--------------------+------+-------+---------------+\n",
      "|2019-06-01 19:17:...|{null, bac5188f-6...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 3a3eb23a-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, f06eb89c-a...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, f0b87796-b...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, e641b45f-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 9152c5d8-7...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 949798fc-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 9d6a8840-d...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 3b0640d6-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 98c8b8cb-7...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, d6880741-a...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, d759fc35-b...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 49c02de6-2...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, b4dac3b4-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 46952066-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, ae413d00-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, a38edd77-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 804ac7f0-c...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 46f718d4-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 8efb00c5-3...|update|vehicle|         org-id|\n",
      "+--------------------+--------------------+------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-------+---------------+\n",
      "|                  at|                data| event|     on|organization_id|\n",
      "+--------------------+--------------------+------+-------+---------------+\n",
      "|2019-06-01 19:17:...|{null, bac5188f-6...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 3a3eb23a-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, f06eb89c-a...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, f0b87796-b...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, e641b45f-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 9152c5d8-7...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 949798fc-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 9d6a8840-d...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 3b0640d6-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 98c8b8cb-7...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, d6880741-a...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, d759fc35-b...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 49c02de6-2...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, b4dac3b4-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 46952066-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, ae413d00-f...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, a38edd77-0...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 804ac7f0-c...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 46f718d4-5...|update|vehicle|         org-id|\n",
      "|2019-06-01 19:17:...|{null, 8efb00c5-3...|update|vehicle|         org-id|\n",
      "+--------------------+--------------------+------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vehicle = df.where(df.on == \"vehicle\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timestamp_format = \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"\n",
    "df = spark.read.option(\"mergeSchema\", \"true\").json(\"/tmp/loka-data/*\")\n",
    "df = (\n",
    "    df.withColumn(\"at\", F.to_timestamp(df.at, timestamp_format))\n",
    "    .withColumn(\"date_start\", F.to_timestamp(df.data.start, timestamp_format))\n",
    "    .withColumn(\"date_finish\", F.to_timestamp(df.data.finish, timestamp_format))\n",
    "    .withColumn(\n",
    "        \"location_at\", F.to_timestamp(df.data.location.at, timestamp_format)\n",
    "    )\n",
    "    .withColumn(\"location_lat\", df.data.location.lat)\n",
    "    .withColumn(\"location_lng\", df.data.location.lng)\n",
    "    .withColumn(\n",
    "        \"data\",\n",
    "        F.struct(\n",
    "            \"data.*\",\n",
    "            \"date_start\",\n",
    "            \"date_finish\",\n",
    "            \"location_at\",\n",
    "            \"location_lat\",\n",
    "            \"location_lng\",\n",
    "        ),\n",
    "    )\n",
    "    .drop(\"date_start\")\n",
    "    .drop(\"date_finish\")\n",
    "    .drop(\"location_at\")\n",
    "    .drop(\"location_lat\")\n",
    "    .drop(\"location_lng\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+--------------------+--------------------+--------------------+-----------+------------+------------+\n",
      "|              finish|  id|location|               start|          date_start|         date_finish|location_at|location_lat|location_lng|\n",
      "+--------------------+----+--------+--------------------+--------------------+--------------------+-----------+------------+------------+\n",
      "|2019-06-01T18:28:...|op_1|    null|2019-06-01T18:23:...|2019-06-01 19:23:...|2019-06-01 19:28:...|       null|        null|        null|\n",
      "|2019-06-01T18:22:...|op_2|    null|2019-06-01T18:17:...|2019-06-01 19:17:...|2019-06-01 19:22:...|       null|        null|        null|\n",
      "+--------------------+----+--------+--------------------+--------------------+--------------------+-----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.on == \"operating_period\").select(\"data.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------------+---------------+-------+--------------------+--------------------+\n",
      "|                  at| event|              on|organization_id|data_id|          date_start|         date_finish|\n",
      "+--------------------+------+----------------+---------------+-------+--------------------+--------------------+\n",
      "|2019-06-01 19:17:...|create|operating_period|         org-id|   op_1|2019-06-01 19:23:...|2019-06-01 19:28:...|\n",
      "|2019-06-01 19:17:...|create|operating_period|         org-id|   op_2|2019-06-01 19:17:...|2019-06-01 19:22:...|\n",
      "+--------------------+------+----------------+---------------+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.on == \"operating_period\")\\\n",
    "    .withColumn(\"data_id\", df.data.id)\\\n",
    "    .withColumn(\"date_start\", df.data.date_start)\\\n",
    "    .withColumn(\"date_finish\", df.data.date_finish)\\\n",
    ".drop(df.data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, udf, struct\n",
    "from pyspark.sql.types import FloatType\n",
    "from sqlalchemy import create_engine\n",
    "raw_data_bucket = \"de-tech-assessment-2022\"\n",
    "raw_data_prefix = \"data\"\n",
    "timestamp_format = \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Loka Application\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"mergeSchema\", \"true\").json(\"/tmp/loka-data/*\")\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"at\", to_timestamp(df.at, timestamp_format))\n",
    "    .withColumn(\"date_start\", to_timestamp(df.data.start, timestamp_format))\n",
    "    .withColumn(\"date_finish\", to_timestamp(df.data.finish, timestamp_format))\n",
    "    .withColumn(\"location_at\", to_timestamp(df.data.location.at, timestamp_format))\n",
    "    .withColumn(\n",
    "        \"data\",\n",
    "        struct(\n",
    "            \"data.*\",\n",
    "            \"date_start\",\n",
    "            \"date_finish\",\n",
    "            \"location_at\",\n",
    "        ),\n",
    "    )\n",
    "    .drop(\"date_start\")\n",
    "    .drop(\"date_finish\")\n",
    "    .drop(\"location_at\")\n",
    ")\n",
    "df_vehicle = (\n",
    "    df.where(df.on == \"vehicle\")\n",
    "    .withColumn(\"data_id\", df.data.id)\n",
    "    .withColumn(\"location_at\", df.data.location_at)\n",
    "    .withColumn(\"location_lat\", df.data.location.lat)\n",
    "    .withColumn(\"location_lng\", df.data.location.lng)\n",
    "    .drop(df.data)\n",
    ")\n",
    "df_operating_period = (\n",
    "    df.where(df.on == \"operating_period\")\n",
    "    .withColumn(\"data_id\", df.data.id)\n",
    "    .withColumn(\"date_start\", df.data.date_start)\n",
    "    .withColumn(\"date_finish\", df.data.date_finish)\n",
    "    .drop(df.data)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilson/.pyenv/versions/3.10.9/envs/loka-3.10/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/nilson/.pyenv/versions/3.10.9/envs/loka-3.10/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pdf_vehicle = df_vehicle.toPandas()\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(\n",
    "    \"postgresql+psycopg2://datawarehouse:datawarehouse@localhost/datawarehouse?client_encoding=utf8\"\n",
    ")\n",
    "# Save result to the database via engine\n",
    "pdf_vehicle.to_sql(\"vehicle\", engine, index=False, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilson/.pyenv/versions/3.10.9/envs/loka-3.10/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/nilson/.pyenv/versions/3.10.9/envs/loka-3.10/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/nilson/.pyenv/versions/3.10.9/envs/loka-3.10/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_operating_period = df_operating_period.toPandas()\n",
    "pdf_operating_period.to_sql(\"operating_period\", engine, index=False, if_exists=\"append\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loka-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd86db1beab7dc41546b92caa68f1741df8a1461e1e110bf2bf3703c47a8662a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
